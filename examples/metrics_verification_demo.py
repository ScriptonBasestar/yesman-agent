#!/usr/bin/env python3
"""Copyright notice."""
# Copyright (c) 2024 Yesman Claude Project
# Licensed under the MIT License

"""Demo script for multi-agent system metrics verification."""

import asyncio
import tempfile
from pathlib import Path

from libs.multi_agent.agent_pool import AgentPool
from libs.multi_agent.metrics_verifier import (
    MetricsVerifier,
)


async def demo_metrics_verification():
    """Demonstrate comprehensive metrics verification system."""
    print("üéØ Multi-Agent System Metrics Verification Demo")
    print("=" * 60)

    # Create temporary work directory
    with tempfile.TemporaryDirectory() as tmpdir:
        work_dir = Path(tmpdir)

        print(f"üìÅ Working directory: {work_dir}")

        # 1. Initialize agent pool and metrics verifier
        print("\n1Ô∏è‚É£ Initializing Multi-Agent System...")
        agent_pool = AgentPool(max_agents=3, work_dir=str(work_dir / "agents"))
        verifier = MetricsVerifier(work_dir=str(work_dir / "metrics"))

        print(f"   ü§ñ Agent pool: {agent_pool.max_agents} max agents")
        print(f"   üìä Metrics directory: {verifier.work_dir}")

        # 2. Show success criteria
        print("\n2Ô∏è‚É£ Success Criteria Requirements...")
        criteria = verifier.success_criteria
        print(f"   üöÄ Speed improvement: {criteria.min_speed_improvement}x - {criteria.max_speed_improvement}x")
        print(f"   üîß Conflict resolution: ‚â•{criteria.min_conflict_resolution_rate:.0%}")
        print(f"   üåø Merge success rate: ‚â•{criteria.min_merge_success_rate:.0%}")
        print(f"   üìà Quality maintenance: ‚â•{criteria.min_quality_maintenance:+.1f}")

        # 3. Get benchmark tasks
        print("\n3Ô∏è‚É£ Benchmark Task Configuration...")
        benchmark_tasks = verifier.get_benchmark_tasks()
        print(f"   üìã Total benchmark tasks: {len(benchmark_tasks)}")
        for i, task in enumerate(benchmark_tasks, 1):
            print(f"      {i}. {task['title']}: {task['description']}")

        # 4. Run single-agent baseline
        print("\n4Ô∏è‚É£ Single-Agent Performance Baseline...")
        print("   ‚è±Ô∏è Running single-agent benchmark (this may take a moment)...")

        single_time = await verifier.measure_single_agent_performance(
            agent_pool=agent_pool,
            benchmark_tasks=benchmark_tasks,
            iterations=2,  # Reduced for demo
        )

        print(f"   ‚úÖ Single-agent baseline: {single_time:.2f} seconds")

        # 5. Run multi-agent performance test
        print("\n5Ô∏è‚É£ Multi-Agent Performance Measurement...")
        print("   ‚è±Ô∏è Running multi-agent benchmark...")

        multi_time = await verifier.measure_multi_agent_performance(
            agent_pool=agent_pool,
            benchmark_tasks=benchmark_tasks,
            iterations=2,  # Reduced for demo
        )

        print(f"   ‚úÖ Multi-agent performance: {multi_time:.2f} seconds")
        print(f"   üìà Speed improvement: {verifier.current_metrics.speed_improvement_ratio:.2f}x")

        # 6. Simulate real-world metrics
        print("\n6Ô∏è‚É£ Simulating Real-World Operation Metrics...")

        # Conflict resolution simulation
        print("   üîß Simulating conflict resolution scenarios...")
        conflicts_scenarios = [
            (10, 9),  # 90% success rate
            (15, 12),  # 80% success rate
            (8, 7),  # 87.5% success rate
        ]

        for total, resolved in conflicts_scenarios:
            verifier.track_conflict_resolution(total, resolved)
            print(f"      Resolved {resolved}/{total} conflicts")

        final_conflicts = verifier.current_metrics
        print(f"   üìä Overall conflict resolution: {final_conflicts.conflict_resolution_rate:.1%}")

        # Branch merge simulation
        print("   üåø Simulating branch merge operations...")
        merge_scenarios = [
            (20, 20),  # 100% success
            (15, 15),  # 100% success
            (10, 10),  # 100% success
            (5, 4),  # 80% success (one failure)
        ]

        for total, successful in merge_scenarios:
            verifier.track_merge_success(total, successful)
            print(f"      Merged {successful}/{total} branches successfully")

        final_merges = verifier.current_metrics
        print(f"   üìä Overall merge success: {final_merges.merge_success_rate:.1%}")

        # Code quality simulation
        print("   üìà Simulating code quality assessment...")
        quality_scenarios = [
            (8.2, 8.4),  # +0.2 improvement
            (7.8, 8.1),  # +0.3 improvement
            (8.5, 8.6),  # +0.1 improvement
        ]

        for initial, final in quality_scenarios:
            verifier.track_code_quality(initial, final)
            print(f"      Quality: {initial} -> {final} (change: {final - initial:+.1f})")

        final_quality = verifier.current_metrics
        print(f"   üìä Overall quality improvement: {final_quality.quality_improvement:+.2f}")

        # 7. Verify success criteria
        print("\n7Ô∏è‚É£ Success Criteria Verification...")
        verification_results = verifier.verify_success_criteria()
        compliance = verification_results["compliance"]

        print("   üéØ Verification Results:")
        print(f"      Speed improvement: {'‚úÖ PASS' if compliance['speed_improvement'] else '‚ùå FAIL'}")
        print(f"      Conflict resolution: {'‚úÖ PASS' if compliance['conflict_resolution'] else '‚ùå FAIL'}")
        print(f"      Merge success: {'‚úÖ PASS' if compliance['merge_success'] else '‚ùå FAIL'}")
        print(f"      Quality maintenance: {'‚úÖ PASS' if compliance['quality_maintenance'] else '‚ùå FAIL'}")
        print(f"      Overall success: {'‚úÖ PASS' if compliance['overall_success'] else '‚ùå FAIL'}")

        # 8. Generate detailed report
        print("\n8Ô∏è‚É£ Detailed Performance Report...")
        report = verifier.generate_performance_report()
        print("\n" + "‚îÄ" * 60)
        print(report)
        print("‚îÄ" * 60)

        # 9. Show verification files
        print("\n9Ô∏è‚É£ Generated Verification Files...")
        metrics_files = list(verifier.work_dir.glob("*.json"))
        for file_path in sorted(metrics_files):
            print(f"   üìÑ {file_path.name}: {file_path.stat().st_size} bytes")

        # 10. Success summary
        print("\nüîü Verification Summary...")
        overall_success = verification_results["overall_success"]

        if overall_success:
            print("   üéâ SUCCESS: Multi-agent system meets all success criteria!")
            print("   ‚úÖ The system achieves the required performance targets:")
            print(f"      ‚Ä¢ {verifier.current_metrics.speed_improvement_ratio:.1f}x speed improvement (target: 2-3x)")
            print(f"      ‚Ä¢ {final_conflicts.conflict_resolution_rate:.0%} conflict resolution rate (target: ‚â•80%)")
            print(f"      ‚Ä¢ {final_merges.merge_success_rate:.0%} merge success rate (target: ‚â•99%)")
            print(f"      ‚Ä¢ {final_quality.quality_improvement:+.1f} quality improvement (target: ‚â•0)")
        else:
            print("   ‚ö†Ô∏è  WARNING: System does not meet all success criteria")
            print("   üìã Areas needing improvement:")

            if not compliance["speed_improvement"]:
                print(f"      ‚Ä¢ Speed improvement: {verifier.current_metrics.speed_improvement_ratio:.1f}x (needs 2-3x)")
            if not compliance["conflict_resolution"]:
                print(f"      ‚Ä¢ Conflict resolution: {final_conflicts.conflict_resolution_rate:.0%} (needs ‚â•80%)")
            if not compliance["merge_success"]:
                print(f"      ‚Ä¢ Merge success: {final_merges.merge_success_rate:.0%} (needs ‚â•99%)")
            if not compliance["quality_maintenance"]:
                print(f"      ‚Ä¢ Quality change: {final_quality.quality_improvement:+.1f} (needs ‚â•0)")

        # 11. Integration recommendations
        print("\n1Ô∏è‚É£1Ô∏è‚É£ Integration Recommendations...")
        print("   üîß To use metrics verification in production:")
        print("   1. Initialize MetricsVerifier in your agent pool setup")
        print("   2. Call verifier.track_*() methods during operations")
        print("   3. Run verifier.verify_success_criteria() periodically")
        print("   4. Use verifier.generate_performance_report() for monitoring")
        print("   5. Store metrics data for historical analysis")

        print("   üìä Automatic integration points:")
        print("      ‚Ä¢ AgentPool task completion callbacks")
        print("      ‚Ä¢ BranchManager conflict resolution events")
        print("      ‚Ä¢ Merge operation success/failure tracking")
        print("      ‚Ä¢ Code quality assessment tools")

        print("\n‚úÖ Metrics Verification Demo completed successfully!")
        print(f"\nüíæ All verification data saved to: {verifier.work_dir}")

        return verification_results


if __name__ == "__main__":
    asyncio.run(demo_metrics_verification())
